<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.27">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>session_03_quali_coding</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="session_03_quali_coding_files/libs/clipboard/clipboard.min.js"></script>
<script src="session_03_quali_coding_files/libs/quarto-html/quarto.js" type="module"></script>
<script src="session_03_quali_coding_files/libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="session_03_quali_coding_files/libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="session_03_quali_coding_files/libs/quarto-html/popper.min.js"></script>
<script src="session_03_quali_coding_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="session_03_quali_coding_files/libs/quarto-html/anchor.min.js"></script>
<link href="session_03_quali_coding_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="session_03_quali_coding_files/libs/quarto-html/quarto-syntax-highlighting-ed96de9b727972fe78a7b5d16c58bf87.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="session_03_quali_coding_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="session_03_quali_coding_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="session_03_quali_coding_files/libs/bootstrap/bootstrap-9e3ffae467580fdb927a41352e75a2e0.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">


</head>

<body class="fullcontent quarto-light">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">




<section id="session-03-qualitative-coding-with-llms" class="level1">
<h1>Session 03 — Qualitative Coding with LLMs</h1>
<section id="session-goal" class="level2">
<h2 class="anchored" data-anchor-id="session-goal">Session goal</h2>
<p>By the end of this session, you should be able to:</p>
<ul>
<li>apply <strong>embeddings</strong> to qualitative coding workflows,</li>
<li>filter transcript chunks by <strong>relevance</strong> to a research question,</li>
<li>classify chunks using a <strong>pre-defined codebook</strong> (theme list),</li>
<li>understand when to use <strong>LLM-based coding</strong> vs.&nbsp;embeddings,</li>
<li>explore <strong>inductive patterns</strong> through clustering.</li>
</ul>
<blockquote class="blockquote">
<p>This session applies the foundational concepts from Session 02 to real qualitative research workflows.</p>
<p><strong>Important:</strong> This is a teaching workflow. In real projects, you should agree on analytic choices with your PI / research team.</p>
</blockquote>
<hr>
</section>
<section id="key-concepts" class="level2">
<h2 class="anchored" data-anchor-id="key-concepts">Key concepts</h2>
<section id="qualitative-coding-with-embeddings" class="level3">
<h3 class="anchored" data-anchor-id="qualitative-coding-with-embeddings">Qualitative coding with embeddings</h3>
<p>Traditional qualitative coding is time-intensive:</p>
<ul>
<li>Read through transcripts</li>
<li>Identify relevant passages</li>
<li>Apply codes from a codebook</li>
<li>Track inter-rater reliability</li>
</ul>
<p><strong>With embeddings, you can:</strong></p>
<ul>
<li>Automatically identify relevant passages</li>
<li>Classify text by semantic similarity to theme definitions</li>
<li>Scale to larger datasets</li>
<li>Maintain consistency and reproducibility</li>
</ul>
<p><strong>Key insight:</strong> Embeddings don’t replace human judgment—they augment and scale it.</p>
<hr>
</section>
<section id="the-qualitative-coding-workflow" class="level3">
<h3 class="anchored" data-anchor-id="the-qualitative-coding-workflow">The qualitative coding workflow</h3>
<pre class="text"><code>Interview transcript
↓
Parse speakers and their statements
↓
Group responses by moderator questions
↓
Generate embeddings for each chunk (question + responses)
↓
[Optional] Filter by relevance to research question
↓
Compare chunks to theme definitions
↓
Assign codes based on similarity scores
↓
Review and validate results</code></pre>
<hr>
</section>
<section id="relevance-filtering" class="level3">
<h3 class="anchored" data-anchor-id="relevance-filtering">Relevance filtering</h3>
<p><strong>Problem:</strong> Interview transcripts cover many topics. You may only care about specific research questions.</p>
<p><strong>Solution:</strong> Embed your research question and compare it to each chunk. Keep only chunks above a similarity threshold.</p>
<p><strong>Example:</strong></p>
<pre class="text"><code>Research question: "How do participants seek help when facing challenges?"
↓
Embed question → [0.123, -0.456, 0.789, ...]
↓
Compare to all chunks
↓
Keep chunks with similarity &gt; 0.4</code></pre>
<p><strong>Benefits:</strong></p>
<ul>
<li>Reduces noise in subsequent analysis</li>
<li>Focuses coding effort on relevant content</li>
<li>Makes theme classification more accurate</li>
</ul>
<hr>
</section>
<section id="theme-classification" class="level3">
<h3 class="anchored" data-anchor-id="theme-classification">Theme classification</h3>
<p><strong>Goal:</strong> Assign codes from a predefined codebook to relevant chunks.</p>
<p><strong>How it works:</strong></p>
<ol type="1">
<li>Define themes as text descriptions (your codebook)</li>
<li>Generate embeddings for each theme definition</li>
<li>For each chunk, compute similarity to all themes</li>
<li>Assign based on your classification rule</li>
</ol>
<p><strong>Classification strategies:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 27%">
<col style="width: 35%">
<col style="width: 37%">
</colgroup>
<thead>
<tr class="header">
<th>Strategy</th>
<th>When to use</th>
<th>How it works</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Single-label (argmax)</strong></td>
<td>Each chunk has one primary theme</td>
<td>Assign the theme with highest similarity</td>
</tr>
<tr class="even">
<td><strong>Multi-label (threshold)</strong></td>
<td>Chunks can have multiple themes</td>
<td>Assign all themes above a threshold (e.g., 0.5)</td>
</tr>
<tr class="odd">
<td><strong>Continuous scores</strong></td>
<td>Themes are dimensions, not categories</td>
<td>Use similarity scores as covariates in analysis</td>
</tr>
<tr class="even">
<td><strong>Exploratory clustering</strong></td>
<td>No predefined themes</td>
<td>Group similar chunks and review clusters</td>
</tr>
</tbody>
</table>
<hr>
</section>
<section id="when-to-use-llm-coding-vs.-embeddings" class="level3">
<h3 class="anchored" data-anchor-id="when-to-use-llm-coding-vs.-embeddings">When to use LLM coding vs.&nbsp;embeddings</h3>
<table class="caption-top table">
<colgroup>
<col style="width: 31%">
<col style="width: 31%">
<col style="width: 18%">
<col style="width: 18%">
</colgroup>
<thead>
<tr class="header">
<th>Approach</th>
<th>Best for</th>
<th>Pros</th>
<th>Cons</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Embeddings</strong></td>
<td>Scale, consistency, predefined codes</td>
<td>Fast, reproducible, cheap</td>
<td>May miss nuance</td>
</tr>
<tr class="even">
<td><strong>LLM coding</strong></td>
<td>Nuanced judgment, binary decisions</td>
<td>Flexible, can explain reasoning</td>
<td>Slower, more expensive</td>
</tr>
<tr class="odd">
<td><strong>Hybrid</strong></td>
<td>Complex coding with scale needs</td>
<td>Combines strengths</td>
<td>Requires more setup</td>
</tr>
</tbody>
</table>
<p><strong>Rule of thumb:</strong></p>
<ul>
<li>Use embeddings for <strong>classification</strong> (which theme?)</li>
<li>Use LLMs for <strong>judgment</strong> (does this mention X? what’s the tone?)</li>
</ul>
<hr>
</section>
</section>
<section id="guided-activities" class="level2">
<h2 class="anchored" data-anchor-id="guided-activities">Guided activities</h2>
<section id="preparation" class="level3">
<h3 class="anchored" data-anchor-id="preparation">Preparation</h3>
<p>Make sure you have:</p>
<ul>
<li>create environment (<code>just venv</code>)</li>
<li><code>.env</code> with <code>OPENAI_API_KEY</code></li>
<li>the repo open in VS Code</li>
<li>completed Session 02 (embeddings basics)</li>
<li>activate environment: <code>.venv/Scripts/activate.ps1</code></li>
</ul>
<hr>
</section>
<section id="activity-a-create-embeddings-for-transcript" class="level3">
<h3 class="anchored" data-anchor-id="activity-a-create-embeddings-for-transcript">Activity A — Create embeddings for transcript</h3>
<p><strong>Optional first step:</strong> If working with transcripts in languages other than English and you want to translate them, run:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb3"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> examples/01_translate_transcript.py</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<blockquote class="blockquote">
<p><strong>Note:</strong> You can also work directly in the original language (e.g., code in Spanish without translating). Embeddings work in multiple languages.</p>
</blockquote>
<p>Then create embeddings:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb4"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> examples/02_create_embeddings.py</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>What this script does:</strong></p>
<ol type="1">
<li>Loads a sample transcript (Spanish focus group interview)</li>
<li><strong>Parses speakers</strong>: Identifies each speaker (MODERADOR, FACILITADOR 1, etc.) and what they said</li>
<li><strong>Groups by moderator questions</strong>: Creates chunks where each chunk contains:
<ul>
<li>The moderator’s question</li>
<li>All participant responses to that question</li>
</ul></li>
<li>Generates embeddings for each chunk</li>
<li>Saves results to <code>outputs/01_chunks_with_embeddings.csv</code></li>
</ol>
<p><strong>Chunking strategy for focus groups:</strong></p>
<p>Instead of splitting by paragraphs or fixed character counts, this script uses a <strong>context-aware chunking</strong> approach:</p>
<ul>
<li>Each moderator question starts a new chunk</li>
<li>All participant responses following that question are grouped together</li>
<li>This preserves the question-response context for better semantic analysis</li>
</ul>
<p><strong>Example chunk:</strong></p>
<pre class="text"><code>MODERADOR: What challenges did you face implementing the program?

FACILITADOR 1: The main challenge was coordinating schedules with families...
FACILITADOR 2: I found it difficult to adapt materials for different age groups...
FACILITADOR 3: Time constraints were our biggest issue...</code></pre>
<p><strong>What to observe:</strong></p>
<ul>
<li>How many chunks were created? (One per moderator question)</li>
<li>What does the embedding column look like?</li>
<li>How large is each embedding vector? (1536 dimensions for text-embedding-3-small)</li>
</ul>
<p><strong>What’s happening:</strong></p>
<ul>
<li>The script parses the transcript structure (speaker labels with colons)</li>
<li>Groups participant responses under each moderator question</li>
<li>Each combined chunk becomes a point in semantic space</li>
<li>Similar question-response patterns will have nearby vectors</li>
<li>This is a one-time process—embeddings are stored for reuse</li>
</ul>
<p><strong>Key insight:</strong> Context-aware chunking (by moderator questions) is more appropriate for focus group data than arbitrary paragraph splits. This preserves the conversational structure and improves downstream coding accuracy.</p>
<hr>
</section>
<section id="activity-b-relevance-filtering-question-focused-approach" class="level3">
<h3 class="anchored" data-anchor-id="activity-b-relevance-filtering-question-focused-approach">Activity B — Relevance filtering (question-focused approach)</h3>
<p>Run:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb6"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> examples/03_relevance_filtering.py</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>What this script does:</strong></p>
<ol type="1">
<li>Loads the chunks with embeddings from Activity A</li>
<li>Embeds a research question: <em>“What helped facilitators integrate Bloom with Love into existing family services?”</em></li>
<li>Computes relevance score (similarity) for each chunk</li>
<li>Shows top relevant chunks and filters by threshold</li>
</ol>
<p><strong>When to use this approach:</strong></p>
<p>Use relevance filtering when you want to <strong>answer a specific research question</strong>. This narrows your dataset to only chunks semantically related to your question, filtering out irrelevant content.</p>
<p><strong>What to observe:</strong></p>
<ul>
<li>Which chunks score highest for relevance?</li>
<li>Are there irrelevant chunks being filtered out?</li>
<li>What happens if you change the threshold?</li>
</ul>
<p><strong>What’s happening:</strong></p>
<pre class="text"><code>Research question embedding: [0.12, -0.45, ...]
↓
Chunk 1 similarity: 0.65 ✅ Relevant
Chunk 2 similarity: 0.23 ❌ Filtered out
Chunk 3 similarity: 0.71 ✅ Relevant
...</code></pre>
<p><strong>Key insight:</strong> This is semantic search applied to qualitative coding—you’re finding meaning matches, not keyword matches. Use this when you have a focused research question and want to filter your data before deeper analysis.</p>
<p><strong>Try this:</strong> Modify the research question and see how the relevant chunks change.</p>
<hr>
</section>
<section id="activity-c-theme-classification-with-embeddings-deductive-coding" class="level3">
<h3 class="anchored" data-anchor-id="activity-c-theme-classification-with-embeddings-deductive-coding">Activity C — Theme classification with embeddings (deductive coding)</h3>
<p>Run:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb8"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> examples/04_theme_classification_embeddings.py</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>What this script does:</strong></p>
<ol type="1">
<li>Loads <strong>all chunks</strong> from Activity A (not filtered by research question)</li>
<li>Loads a predefined theme list (<code>data/themes/help_themes.json</code>) — your codebook</li>
<li>Embeds each theme definition</li>
<li>For each chunk, computes similarity to all themes</li>
<li>Assigns the best-matching theme (argmax strategy)</li>
<li>Saves results to CSV and generates an interactive HTML report</li>
<li>Shows examples of coded chunks per theme in console</li>
</ol>
<p><strong>Outputs generated:</strong></p>
<ul>
<li><strong>CSV file</strong>: <code>outputs/03_theme_classification.csv</code> — Full data with all similarity scores and theme assignments</li>
<li><strong>HTML report</strong>: <code>outputs/03_theme_classification_report.html</code> — Interactive visualization:
<ul>
<li>Summary cards showing chunk count and average score per theme</li>
<li>Expandable/collapsible sections for each theme</li>
<li>Top 10 examples per theme with similarity scores</li>
<li>Easy navigation and visual layout</li>
</ul></li>
<li><strong>Console output</strong>: Enhanced statistics and top 3 examples per theme</li>
</ul>
<p><strong>To view the HTML report:</strong> Open <code>outputs/03_theme_classification_report.html</code> in your web browser (double-click the file or right-click → Open with → Browser)</p>
<p><strong>When to use this approach:</strong></p>
<p>Use theme classification when you want <strong>traditional deductive coding</strong>: you have a predefined codebook (theme dictionary) and want to classify all your data according to those codes. This is similar to manual coding with a predefined set of codes, but automated using semantic similarity.</p>
<p><strong>Key difference from Activity B:</strong></p>
<ul>
<li><strong>Activity B (relevance filtering):</strong> Filters data by ONE research question → narrows dataset</li>
<li><strong>Activity C (theme classification):</strong> Applies MULTIPLE codes to all data → full codebook classification</li>
</ul>
<p><strong>What to observe:</strong></p>
<ul>
<li>Do the assigned themes make sense?</li>
<li>Are there themes that overlap too much?</li>
<li>How confident are the assignments (similarity scores)?</li>
<li>Which themes have the most/fewest chunks?</li>
<li>Are the top-scoring examples truly representative of each theme?</li>
</ul>
<p><strong>What’s happening:</strong></p>
<pre class="text"><code>Themes:
  - "Seeking help from family and friends"
  - "Professional help (doctors, counselors)"
  - "Community resources and organizations"
  
Chunk: "I talked to my sister and she gave me advice..."
↓
Similarity to Theme 1: 0.78 ← Assigned ✅
Similarity to Theme 2: 0.34
Similarity to Theme 3: 0.29</code></pre>
<p><strong>Key insight:</strong> Theme definitions matter! More specific, distinct definitions lead to better classification.</p>
<p><strong>Discussion questions:</strong></p>
<ul>
<li>Which chunks were hard to classify?</li>
<li>Should we use multi-label classification instead?</li>
<li>Are there missing themes in our codebook?</li>
</ul>
<hr>
</section>
<section id="activity-d-extract-themes-with-llm-inductive-coding-optional" class="level3">
<h3 class="anchored" data-anchor-id="activity-d-extract-themes-with-llm-inductive-coding-optional">Activity D — Extract themes with LLM (inductive coding, optional)</h3>
<p>Run:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb10"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> examples/05_extract_themes_llm.py</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>What this script does:</strong></p>
<ol type="1">
<li>Loads the full transcript (translated English if available, or Spanish original)</li>
<li>Sends the entire transcript to the LLM without a specific research question</li>
<li>Asks the LLM to identify 8-15 recurring themes across all discussions</li>
<li>For each theme, generates:
<ul>
<li>Clear, concise theme name</li>
<li>Detailed definition (1-2 sentences)</li>
<li>Key examples or quotes that illustrate the theme</li>
</ul></li>
<li>Saves results to <code>outputs/04_extracted_themes.txt</code></li>
</ol>
<p><strong>When to use this approach:</strong></p>
<p>Use LLM theme extraction when you <strong>don’t have a predefined codebook</strong> and want to perform <strong>inductive coding</strong>. This is exploratory analysis where themes emerge from the data rather than being defined in advance.</p>
<p><strong>Key difference from Activity C:</strong></p>
<ul>
<li><strong>Activity C (deductive):</strong> You define themes first → classify chunks by those themes</li>
<li><strong>Activity D (inductive):</strong> LLM reads all data → generates/discovers themes from patterns</li>
</ul>
<p><strong>What to observe:</strong></p>
<ul>
<li>How do LLM-generated themes compare to your predefined ones (from Activity C)?</li>
<li>Are the themes specific enough or too broad?</li>
<li>Do the themes capture the breadth of discussion in the transcript?</li>
<li>Which approach would you prefer for your research: predefined or LLM-extracted themes?</li>
</ul>
<p><strong>What’s happening:</strong></p>
<ul>
<li>The LLM reads through the entire transcript and identifies patterns</li>
<li>It generates theme names and definitions based on what participants discussed</li>
<li>This can serve as a starting point for codebook development</li>
<li>You would typically review, refine, and validate these themes with your research team</li>
</ul>
<p><strong>Use case:</strong> When you don’t have a predefined codebook and need exploratory theme identification.</p>
<p><strong>Key insight:</strong> LLMs can help with inductive coding, but themes still need researcher validation.</p>
<hr>
</section>
<section id="activity-e-non-verbal-cue-coding-optional" class="level3">
<h3 class="anchored" data-anchor-id="activity-e-non-verbal-cue-coding-optional">Activity E — Non-verbal cue coding (optional)</h3>
<p>Run:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb11"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> examples/06_nonverbal_coding_llm.py</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>What this script does:</strong></p>
<ol type="1">
<li>Loads <strong>all chunks</strong> from Activity A (entire transcript)</li>
<li>For each chunk, asks the LLM to detect non-verbal cues (laughter, pauses, tone changes, etc.)</li>
<li>Extracts structured codes: presence of cues (YES/NO) and type of cue</li>
<li>Generates an interactive HTML report and CSV with results</li>
<li>Saves to <code>outputs/05_nonverbal_coding.csv</code> and <code>outputs/05_nonverbal_coding_report.html</code></li>
</ol>
<p><strong>Outputs generated:</strong></p>
<ul>
<li><strong>CSV file</strong>: <code>outputs/05_nonverbal_coding.csv</code> — All chunks with non-verbal cue annotations</li>
<li><strong>HTML report</strong>: <code>outputs/05_nonverbal_coding_report.html</code> — Interactive visualization:
<ul>
<li>Summary statistics (total chunks, chunks with cues, percentage)</li>
<li>Expandable sections grouped by cue type (Laughter, Pauses, Confusion, etc.)</li>
<li>Full context for each chunk containing non-verbal signals</li>
<li>Easy navigation with visual layout</li>
</ul></li>
<li><strong>Console output</strong>: Progress updates, summary statistics, and examples</li>
</ul>
<p><strong>To view the HTML report:</strong> Open <code>outputs/05_nonverbal_coding_report.html</code> in your web browser</p>
<p><strong>What to observe:</strong></p>
<ul>
<li>What kinds of non-verbal information does the LLM identify?</li>
<li>How frequent are non-verbal cues in the transcript?</li>
<li>Are certain types of cues more common in specific discussion topics?</li>
<li>Could embeddings do this? Why or why not?</li>
<li>How reliable are these judgments?</li>
</ul>
<p><strong>What’s happening:</strong></p>
<ul>
<li>The LLM is looking for indicators beyond semantic content</li>
<li>It codes emotional tone, engagement, hesitation, laughter, etc.</li>
<li>This complements thematic coding by capturing affective and interactional dimensions</li>
<li>The script processes all chunks (may take several minutes depending on transcript size)</li>
</ul>
<p><strong>Use case:</strong> When you need to code for affect, engagement, communication style, or group dynamics that go beyond what participants explicitly said.</p>
<p><strong>Key insight:</strong> LLMs can code for meta-communicative features that embeddings can’t capture—but this requires clear instructions and validation. Non-verbal cues can reveal emotional responses, group dynamics, and engagement levels that inform interpretation of thematic content.</p>
<hr>
</section>
<section id="activity-f-inductive-clustering-optional" class="level3">
<h3 class="anchored" data-anchor-id="activity-f-inductive-clustering-optional">Activity F — Inductive clustering (optional)</h3>
<p>Run:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb12"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python</span> examples/07_inductive_clustering.py</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><strong>What this script does:</strong></p>
<ol type="1">
<li>Loads <strong>all chunks</strong> from Activity A (entire transcript with embeddings)</li>
<li>Runs K-Means clustering algorithm to group similar chunks (creates 8 clusters)</li>
<li>Generates t-SNE 2D visualization showing cluster relationships</li>
<li>Saves results to CSV and PNG</li>
<li>Shows example chunks from each cluster</li>
</ol>
<p><strong>Outputs generated:</strong></p>
<ul>
<li><strong>CSV file</strong>: <code>outputs/06_clusters.csv</code> — All chunks with assigned cluster labels</li>
<li><strong>PNG visualization</strong>: <code>outputs/06_clusters_tsne.png</code> — 2D scatter plot of clusters using t-SNE dimensionality reduction</li>
<li><strong>Console output</strong>: Summary statistics, cluster sizes, and representative examples from each cluster</li>
</ul>
<p><strong>When to use this approach:</strong></p>
<p>Use clustering for <strong>exploratory inductive analysis</strong> when you don’t have predefined themes and want to discover natural groupings in your data. The algorithm finds chunks that are semantically similar and groups them together.</p>
<p><strong>What to observe:</strong></p>
<ul>
<li>Do the clusters reveal meaningful patterns?</li>
<li>Are there unexpected groupings that suggest themes you hadn’t considered?</li>
<li>How would you label these clusters as themes?</li>
<li>Do cluster sizes make sense? (Some topics may naturally be discussed more)</li>
<li>Looking at the t-SNE plot, are clusters well-separated or overlapping?</li>
</ul>
<p><strong>What’s happening:</strong></p>
<pre class="text"><code>All chunk embeddings (1,536 dimensions) → K-Means (k=8 clusters)
↓
Cluster 0: [40 chunks] Pattern about [identify from examples]
Cluster 1: [35 chunks] Pattern about [identify from examples]
Cluster 2: [28 chunks] Pattern about [identify from examples]
...
↓
t-SNE reduces to 2D for visualization → PNG plot</code></pre>
<p><strong>How to interpret clusters:</strong></p>
<ol type="1">
<li>Read the example chunks from each cluster</li>
<li>Identify common themes or topics across chunks in that cluster</li>
<li>Give the cluster a descriptive label</li>
<li>These labels become your emergent codebook</li>
<li>Validate with research team</li>
</ol>
<p><strong>Use case:</strong> Exploratory analysis when you don’t have predefined themes. Clustering is especially useful for identifying unexpected patterns or when building a codebook from scratch.</p>
<p><strong>Key insight:</strong> Clustering is hypothesis-generating, not hypothesis-testing. The algorithm finds mathematical patterns in semantic space—you provide the qualitative interpretation. Always review clusters with domain expertise to ensure they’re meaningful, not just mathematically coherent.</p>
<hr>
</section>
</section>
<section id="complete-workflow-example" class="level2">
<h2 class="anchored" data-anchor-id="complete-workflow-example">Complete workflow example</h2>
<p>Here’s how you might combine these tools in a real project:</p>
<p><strong>Scenario:</strong> Analyzing 8 focus groups about facilitators’ experiences integrating a family support program into existing services.</p>
<p><strong>Step 1 — Prepare data:</strong></p>
<ul>
<li>Transcribe focus group recordings</li>
<li>Ensure speaker labels are clear (MODERADOR, FACILITADOR 1, etc.)</li>
<li>Translate if needed → <code>examples/01_translate_transcript.py</code> (optional if working in Spanish/original language)</li>
</ul>
<p><strong>Step 2 — Generate embeddings:</strong></p>
<ul>
<li>Run <code>examples/02_create_embeddings.py</code> on all focus group transcripts</li>
<li>Chunks are created by moderator questions (preserving question-response context)</li>
<li>Store embeddings for reuse → <code>outputs/01_chunks_with_embeddings.csv</code></li>
</ul>
<p><strong>Step 3A — Question-focused approach (if you have a specific research question):</strong></p>
<ul>
<li>Define your research question (e.g., “What helped facilitators integrate Bloom with Love?”)</li>
<li>Run <code>examples/03_relevance_filtering.py</code></li>
<li>Keep only relevant chunks → <code>outputs/02_relevant_chunks.csv</code></li>
<li>Use these filtered chunks for targeted analysis</li>
</ul>
<p><strong>Step 3B — OR comprehensive coding approach (if coding all discussions):</strong></p>
<ul>
<li>Skip relevance filtering</li>
<li>Proceed directly to theme classification with all chunks</li>
</ul>
<p><strong>Step 4 — Classify themes (deductive approach):</strong></p>
<ul>
<li>Have a predefined codebook? → <code>examples/04_theme_classification_embeddings.py</code></li>
<li>Review interactive HTML report (<code>outputs/03_theme_classification_report.html</code>)</li>
<li>Analyze distribution of chunks across themes</li>
<li>Identify representative examples for each theme</li>
</ul>
<p><strong>Step 5 — OR discover themes (inductive approach):</strong></p>
<ul>
<li>No predefined codebook? → <code>examples/05_extract_themes_llm.py</code></li>
<li>LLM generates 8-15 themes from the data</li>
<li>Review extracted themes → <code>outputs/04_extracted_themes.txt</code></li>
<li>Refine theme definitions with research team</li>
<li>Create your codebook from these emergent themes</li>
<li>Then proceed to theme classification (Step 4)</li>
</ul>
<p><strong>Step 6 — Code non-verbal dimensions:</strong></p>
<ul>
<li>Run <code>examples/06_nonverbal_coding_llm.py</code></li>
<li>Identify laughter, pauses, group dynamics → <code>outputs/05_nonverbal_coding.csv</code></li>
<li>Review HTML report for patterns by cue type</li>
<li>Analyze how non-verbal cues relate to discussion topics</li>
<li>Use to inform interpretation of thematic findings</li>
</ul>
<p><strong>Step 7 — Validate and refine:</strong></p>
<ul>
<li>Review coded chunks in HTML reports</li>
<li>Check if theme definitions need adjustment</li>
<li>Look for themes that overlap too much or are too broad</li>
<li>Re-run classification with updated codebook if needed</li>
<li>Compare deductive codes (Activity C) with inductive findings (Activity D &amp; F)</li>
</ul>
<p><strong>Step 8 — Synthesize findings:</strong></p>
<ul>
<li>Export coded data from CSVs</li>
<li>Cross-tabulate themes with participant characteristics</li>
<li>Identify patterns across focus groups</li>
<li>Use representative quotes from HTML reports</li>
<li>Integrate non-verbal codes to interpret engagement and affect</li>
</ul>
<p><strong>Key decision points:</strong></p>
<ul>
<li><strong>Research question-focused vs.&nbsp;comprehensive?</strong> → Determines if you use relevance filtering (Activity B)</li>
<li><strong>Deductive vs.&nbsp;inductive?</strong> → Determines if you start with codebook (Activity C) or generate themes (Activity D/F)</li>
<li><strong>Scale vs.&nbsp;nuance?</strong> → Embeddings for classification, LLMs for complex judgments</li>
<li><strong>Focus group specific:</strong> Non-verbal codes capture group dynamics that individual interviews miss</li>
</ul>
<hr>
</section>
<section id="mental-model-choosing-your-coding-approach" class="level2">
<h2 class="anchored" data-anchor-id="mental-model-choosing-your-coding-approach">Mental model: Choosing your coding approach</h2>
<pre class="text"><code>Do you have a predefined codebook?
├─ YES → Use embedding classification (Activity C)
│   └─ Want to explore patterns? → Also run clustering (Activity F)
└─ NO → Extract themes with LLM (Activity D) OR cluster embeddings (Activity F)
    └─ Then: Validate themes and proceed with classification

Need to code non-verbal cues or meta-features?
└─ Use LLM coding (Activity E)

Have a specific research question?
└─ YES → Filter by relevance first (Activity B), then code</code></pre>
<hr>
</section>
<section id="common-issues-and-solutions" class="level2">
<h2 class="anchored" data-anchor-id="common-issues-and-solutions">Common issues and solutions</h2>
<p><strong>“All chunks get assigned to the same theme”</strong> → Theme definitions might overlap too much. Make them more distinct and specific.</p>
<p><strong>“Relevance filtering removes too many chunks”</strong> → Lower your threshold or rephrase your research question to be broader.</p>
<p><strong>“Clusters don’t make sense”</strong> → Try different values of <code>k</code> (number of clusters). Clustering is exploratory—not all datasets have clear natural groupings. Review example chunks to identify patterns.</p>
<p><strong>“Embedding classification misses subtle cases”</strong> → This is expected. Use hybrid approach: embeddings for scale, complement with manual review of edge cases.</p>
<p><strong>“Costs are adding up with LLM coding”</strong> → Use embeddings for classification (cheap), LLMs only for non-verbal coding or theme extraction. Avoid LLM coding for every chunk.</p>
<p><strong>“HTML reports aren’t opening”</strong> → Make sure you’re opening the .html files in a web browser (Chrome, Firefox, Edge), not in a text editor.</p>
<p><strong>“Non-verbal coding takes too long”</strong> → This is expected—it processes every chunk with an LLM call. For large datasets, consider sampling or running overnight.</p>
<hr>
</section>
<section id="what-youve-learned" class="level2">
<h2 class="anchored" data-anchor-id="what-youve-learned">What you’ve learned</h2>
<ul>
<li>✅ Apply embeddings to qualitative coding workflows</li>
<li>✅ Filter transcript chunks by research question relevance</li>
<li>✅ Classify chunks using theme similarity (codebook-based, deductive)</li>
<li>✅ Extract themes inductively with LLMs</li>
<li>✅ Discover patterns through unsupervised clustering</li>
<li>✅ Code non-verbal and meta-features with LLM judgment</li>
<li>✅ Understand trade-offs: embeddings (scale) vs.&nbsp;LLMs (nuance)</li>
<li>✅ Generate interactive HTML reports for exploration and validation</li>
</ul>
<hr>
</section>
<section id="discussion-prompts" class="level2">
<h2 class="anchored" data-anchor-id="discussion-prompts">Discussion prompts</h2>
<p>Use these to reflect on your coding choices:</p>
<ul>
<li>Which chunks were filtered out as irrelevant? Are we okay with that?</li>
<li>Do the top chunks per theme look right in the HTML reports?</li>
<li>Are there themes that overlap too much or are too broad?</li>
<li>How do deductive codes (Activity C) compare with inductive themes (Activity D &amp; F)?</li>
<li>Do the clusters reveal unexpected patterns we should investigate?</li>
<li>How frequent are non-verbal cues, and what do they tell us about group dynamics?</li>
<li>Would multi-label classification fit our research question better?</li>
<li>How would you validate these codes with traditional qualitative methods?</li>
<li>What would inter-rater reliability look like for embedding-based codes?</li>
</ul>
<hr>
</section>
<section id="key-takeaways" class="level2">
<h2 class="anchored" data-anchor-id="key-takeaways">Key takeaways</h2>
<ol type="1">
<li><strong>Embeddings scale qualitative coding</strong>: Semantic similarity enables automated classification across large datasets</li>
<li><strong>Relevance filtering focuses analysis</strong>: Don’t code everything—code what matters to your research question</li>
<li><strong>Theme definitions matter</strong>: Clear, distinct definitions improve classification accuracy</li>
<li><strong>Deductive vs.&nbsp;inductive</strong>: Choose based on whether you have a predefined codebook or need exploratory analysis</li>
<li><strong>Embeddings vs.&nbsp;LLMs</strong>: Use embeddings for classification (fast, cheap), LLMs for judgment and meta-features (slower, nuanced)</li>
<li><strong>Interactive reports enhance exploration</strong>: HTML visualizations make it easier to validate and interpret results</li>
<li><strong>Validation is still essential</strong>: Tools augment, not replace, researcher judgment and domain expertise</li>
</ol>
<hr>
</section>
<section id="next-steps-and-extensions" class="level2">
<h2 class="anchored" data-anchor-id="next-steps-and-extensions">Next steps and extensions</h2>
<p><strong>If you want to go further:</strong></p>
<ul>
<li><strong>Store embeddings efficiently</strong>: Use a vector database (Pinecone, Weaviate) for large datasets</li>
<li><strong>Track provenance</strong>: Add chunk IDs and source metadata for traceability</li>
<li><strong>Measure agreement</strong>: Compare embedding codes to human codes (Cohen’s kappa)</li>
<li><strong>Multi-label classification</strong>: Implement threshold-based multi-label assignment</li>
<li><strong>Hierarchical themes</strong>: Build theme taxonomies with parent-child relationships</li>
<li><strong>Longitudinal analysis</strong>: Compare themes across waves or time periods</li>
</ul>
<p><strong>Tools to explore:</strong></p>
<ul>
<li><strong>NVivo</strong>: Import embedding-based codes for further qualitative analysis</li>
<li><strong>Atlas.ti</strong>: Export coded chunks with similarity scores</li>
<li><strong>Dedoose</strong>: Integrate with mixed-methods analysis workflows</li>
</ul>
<hr>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>
<ul>
<li>OpenAI Embeddings Best Practices: <a href="https://platform.openai.com/docs/guides/embeddings/use-cases" class="uri">https://platform.openai.com/docs/guides/embeddings/use-cases</a></li>
<li>Qualitative Coding with AI: <a href="https://www.anthropic.com/research/qualitative-coding" class="uri">https://www.anthropic.com/research/qualitative-coding</a></li>
<li>Vector Similarity for Researchers: <a href="https://www.pinecone.io/learn/semantic-search/" class="uri">https://www.pinecone.io/learn/semantic-search/</a></li>
<li>K-Means Clustering Explained: <a href="https://scikit-learn.org/stable/modules/clustering.html#k-means" class="uri">https://scikit-learn.org/stable/modules/clustering.html#k-means</a></li>
</ul>
<hr>
<p><strong>Congratulations!</strong> You’ve completed the hands-on training sessions. You now have the tools to apply LLMs and embeddings to real qualitative research workflows.</p>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>